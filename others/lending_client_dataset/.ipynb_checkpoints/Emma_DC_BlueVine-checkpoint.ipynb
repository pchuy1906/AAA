{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "## Goal\n",
    "The primary goal is to build a model to predict \n",
    "\n",
    "## Plan\n",
    "\n",
    "## Key findings\n",
    "\n",
    "\n",
    "## Future work\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data from csv file\n",
    "data_orig = pd.read_csv('product_ds_exercise_2018_h2_dataset_1_1.csv', header=0)\n",
    "#print('Shape of the original dataset ', data_orig.shape)\n",
    "#data_orig.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of rows with non-null values for each columns \n",
    "data_orig.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_orig.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many null values in each column\n",
    "data_orig.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_orig['signup_os'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning/preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['vehicle_model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e78433f9a1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signup_os'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "set(data['signup_os'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#datetime data:\n",
    "training_orig['PurchDate'] = pd.to_datetime(training_orig['PurchDate'])\n",
    "training_orig['month'] = training_orig['PurchDate'].dt.month\n",
    "training_orig['year'] = training_orig['PurchDate'].dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[data.duplicated(keep=False)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checks the first few duplicates to see if they are exactly duplicate\n",
    "data.iloc[np.where(data.duplicated(keep=False))[0]].head(6)\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[data.duplicated(subset=['id'],keep=False)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for categorical variables\n",
    "column_list = ['Trim', 'SubModel', 'Color','Transmission','WheelTypeID','WheelType', 'Nationality', 'Size', 'TopThreeAmericanName']\n",
    "for column in column_list:\n",
    "    #training_clean.fillna(value={column: training_clean[column].value_counts().index[0]}, inplace = True)\n",
    "    #test_clean.fillna(value={column: training_clean[column].value_counts().index[0]}, inplace = True)\n",
    "\n",
    "    training_clean.fillna(value={column: training_clean[column].mode()[0]}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for numerical variables\n",
    "column_list = ['MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\\\n",
    "               'MMRAcquisitionRetailAveragePrice']\n",
    "for column in column_list:\n",
    "    training_clean.fillna(value={column: training_clean[column].mean()}, inplace = True)\n",
    "    test_clean.fillna(value={column: training_clean[column].mean()}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "marker = ['o', '+', 's']\n",
    "plt.figure(figsize=(6,5))\n",
    "signals = ['position_ned_m[0]','position_ned_m[1]', 'position_ned_m[2]']\n",
    "for idx in range(len(signals)) :\n",
    "    plt.plot(flight_15thSecond.flight_id, flight_15thSecond[signals[idx]], marker = marker[idx])\n",
    "plt.legend(['north','east','down'])\n",
    "plt.xlabel('flight IDs')\n",
    "plt.ylabel('position at 15th second after launch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "plt.figure(figsize=(8,5))\n",
    "flight_15thSecond[['position_ned_m[0]','position_ned_m[1]', 'position_ned_m[2]']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram\n",
    "flight_15thSecond[['position_ned_m[0]','position_ned_m[1]', 'position_ned_m[2]']].hist()\n",
    "plt.show()\n",
    "\n",
    "numerical_features = training_clean.dtypes[training_clean.dtypes != 'object'].index.values\n",
    "plt.figure(1, figsize = (20,10))\n",
    "count = 0\n",
    "for feature in numerical_features[2:9]:\n",
    "    plt.subplot(str(23) + str(count))\n",
    "    count += 1\n",
    "    training_clean[feature].hist()\n",
    "    plt.title('Frequency distribution of '+ feature)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.axvline(x=17136, color='r', linestyle='--', label='outliers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of samples in each class\n",
    "ax = sns.countplot(x=training_clean['IsBadBuy'],label=\"Count\",palette=\"Set2\")       \n",
    "print('number of class 1 samples ', training_clean[training_clean['IsBadBuy'] == 1].shape[0])\n",
    "print('number of class 0 samples ', training_clean[training_clean['IsBadBuy'] == 0].shape[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_clean['IsBadBuy'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of features relative to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing categorical features\n",
    "plt.figure(1)\n",
    "plt.subplot(221)\n",
    "train['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender')\n",
    "\n",
    "plt.subplot(222)\n",
    "train['Married'].value_counts(normalize=True).plot.bar(title= 'Married')\n",
    "\n",
    "plt.subplot(223)\n",
    "train['Dependents'].value_counts(normalize=True).plot.bar(title= 'Dependents')\n",
    "\n",
    "plt.subplot(224)\n",
    "train['Education'].value_counts(normalize=True).plot.bar(title= 'Education')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing numerical features\n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "sns.distplot(train['ApplicantIncome']);\n",
    "\n",
    "plt.subplot(122)\n",
    "train['ApplicantIncome'].plot.box(figsize=(16,5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_features = training_clean.dtypes[training_clean.dtypes == 'object'].index.values\n",
    "for feature in categorical_features:\n",
    "    sns.countplot(x = feature, hue='IsBadBuy', data=training_clean, palette=\"Set2\", \\\n",
    "                 order = training_clean[feature].value_counts().iloc[:10].index);\n",
    "    plt.title('Frequency distribution of '+ feature)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "#############\n",
    "categorical_features = training_clean.dtypes[training_clean.dtypes == 'object'].index.values\n",
    "plt.figure(1, figsize = (20,20))\n",
    "count = 0\n",
    "for feature in categorical_features[:6]:\n",
    "    plt.subplot(str(23) + str(count))\n",
    "    sns.countplot(x = feature, hue='IsBadBuy', data=training_clean, palette=\"Set2\", \\\n",
    "                 order = training_clean[feature].value_counts().iloc[:10].index);\n",
    "    \n",
    "    count += 1\n",
    "    #training_clean[feature].value_counts()[:5].plot.bar()\n",
    "    plt.title('Frequency distribution of '+ feature)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations: features-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_clean[numerical_feature[6]].hist() # hist(bin=20)\n",
    "sns.distplot(training_clean[numerical_feature[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(training_clean[numerical_feature].corr(), cmap=\"BrBG\",annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection/engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replacing 3+ in Dependents variable with 3\n",
    "# replacing Y and N in Loan_Status variable with 1 and 0 respectively\n",
    "train['Dependents'].replace(('0', '1', '2', '3+'), (0, 1, 2, 3),inplace=True)\n",
    "test['Dependents'].replace(('0', '1', '2', '3+'), (0, 1, 2, 3),inplace=True)\n",
    "train['Loan_Status'].replace('N', 0,inplace=True)\n",
    "train['Loan_Status'].replace('Y', 1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting \n",
    "\n",
    "The dataset is split into 80/20 for training and testing. \n",
    "\n",
    "If have more time, I would devide into 3 sets, training-validation-test sets. Hold the test set completely from training and model selection process (which validates on validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_orig.drop('month',axis=1).head(2)\n",
    "X = train.drop('Loan_Status',1)\n",
    "y = train.Loan_Status\n",
    "\n",
    "#====\n",
    "X = training_clean[training_clean.columns[3:]].copy(deep=True)\n",
    "y = training_clean[training_clean.columns[1]].copy()\n",
    "\n",
    "# One hot encoding\n",
    "X_encoded = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "offset_test = int((1-test_size)* X_encoded.shape[0])\n",
    "idx = np.random.permutation(X_encoded.index.values)\n",
    "idx_train, idx_test = idx[:offset_test], idx[offset_test:] \n",
    "\n",
    "X_train, y_train = X_encoded.loc[idx_train], y.loc[idx_train]\n",
    "X_test, y_test = X_encoded.loc[idx_test], y.loc[idx_test]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "This is a binary classification problem. Among the tools that could be used are:\n",
    "1. logistic regression\n",
    "2. support vector machines\n",
    "3. decision trees, random forest\n",
    "\n",
    "\n",
    "I choose logistic regression (LR) to start with for its simplicity, interpretability  and easy to train. Using LR with one hot coding, I am assuming there is a linear relationship between input and target variables, also even spacing between categorical features.  \n",
    "\n",
    "No need to worry about class imbalance here, I aim for accuracy as the target metric. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score, roc_curve\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_test_logreg = logreg.predict(X_test)\n",
    "y_train_logreg = logreg.predict(X_train)\n",
    "print('Accuracy on test set with logistic regression ' ,'%.3f'  %(100*accuracy_score(y_test, y_test_logreg)), '%' )\n",
    "print('Accuracy on training set with logistic regression ' ,'%.3f'  %(100*accuracy_score(y_train, logreg.predict(X_train))), '%' )\n",
    "print('F1-score test set with logistic regression ' ,'%.3f'  %(f1_score(y_test, y_test_logreg)))\n",
    "print('AUC score test set with logistic regression ' ,'%.3f'  %(roc_auc_score(y_test, y_test_logreg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rforest = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "#rforest.fit(X_train, y_train)\n",
    "rforest.fit(data[data.columns[1:10]],data[data.columns[10]])\n",
    "\n",
    "y_val_rforest = rforest.predict(X_val)\n",
    "print('Accuracy on validation set with random forrest ' ,'%.3f'  %(100*accuracy_score(y_val, y_val_rforest)), '%' )\n",
    "print('Accuracy on training set with random forrest ' ,'%.3f'  %(100*accuracy_score(y_train, rforest.predict(X_train))), '%' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "y_train_rforest = rforest.predict_proba(X_train)[:,1] # keep probabilities for the positive outcome only\n",
    "fpr_RF, tpr_RF, _ = roc_curve(y_train, y_train_rforest)\n",
    "auc_RF = roc_auc_score(y_train, y_train_rforest)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(fpr,tpr,label=\"LR, auc=\"+str(auc))\n",
    "plt.plot(fpr_RF,tpr_RF,label=\"RF, auc=\"+str(auc_RF))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,average_precision_score,auc\n",
    "\n",
    "# calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test,  rforest.predict_proba(X_test)[:,1])\n",
    "plt.plot(recall, precision, marker='.')\n",
    "# calculate F1 score\n",
    "f1 = f1_score(y_test, rforest.predict(X_test))\n",
    "# calculate precision-recall AUC\n",
    "auc = auc(recall, precision)\n",
    "# calculate average precision score\n",
    "ap = average_precision_score(y_test,  rforest.predict_proba(X_test)[:,1])\n",
    "# plot no skill\n",
    "plt.plot([0, 1], [0.1, 0.1], linestyle='--')\n",
    "print('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_curve\n",
    "predicted_labels = model.predict(x)\n",
    "plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)\n",
    "plt.scatter(x[y==0,0],x[y==0,1], color='b')\n",
    "plt.scatter(x[y==1,0],x[y==1,1], color='r');\n",
    "fpr, tpr, thresholds = roc_curve(y, predicted_labels)\n",
    "plt.title(\"precision = {:.2f}, recall = {:.2f}, fpr = {:.2f}, tpr = {:.2f}\".format(\n",
    "    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[0], tpr[0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "#fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "#auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "#plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "#plt.legend(loc=4)\n",
    "#plt.show()\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "vdict = {}\n",
    "for ii in range(len(X_encoded.columns)):\n",
    "    vdict[X_encoded.columns[ii]] = rforest.feature_importances_[ii]\n",
    "df_imp = pd.DataFrame.from_dict(vdict, orient='index')\n",
    "\n",
    "# plot weights\n",
    "df_imp.sort_values(by=0).plot.barh(figsize=(8,8))\n",
    "plt.xlabel('Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
