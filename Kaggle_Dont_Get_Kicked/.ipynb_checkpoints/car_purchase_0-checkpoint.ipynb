{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "bc1536a2d8fbce0795614d67d9205b4ab2baf132",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set has  (72983, 34)\n",
      "test set has  (48707, 33)\n",
      "remove one column from the training set\n",
      "merge training set and test set to all_data\n",
      "all_data has  (121690, 33)\n"
     ]
    }
   ],
   "source": [
    "# Read the data into dataframes\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('training.csv')\n",
    "print (\"training set has \", train_df.shape)\n",
    "test_df = pd.read_csv('test.csv')\n",
    "print (\"test set has \", test_df.shape)\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "print (\"remove one column from the training set\")\n",
    "print (\"merge training set and test set to all_data\")\n",
    "\n",
    "Ytrain = train_df['IsBadBuy']\n",
    "train_df = train_df.drop(columns=['IsBadBuy'])\n",
    "\n",
    "# Put all data together so we can wrangle all the data at the same time\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "print (\"all_data has \", all_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RefId                                     0\n",
       "PurchDate                                 0\n",
       "Auction                                   0\n",
       "VehYear                                   0\n",
       "VehicleAge                                0\n",
       "Make                                      0\n",
       "Model                                     0\n",
       "Trim                                   3910\n",
       "SubModel                                 13\n",
       "Color                                    12\n",
       "Transmission                             12\n",
       "WheelTypeID                            5357\n",
       "WheelType                              5362\n",
       "VehOdo                                    0\n",
       "Nationality                              12\n",
       "Size                                     12\n",
       "TopThreeAmericanName                     12\n",
       "MMRAcquisitionAuctionAveragePrice        28\n",
       "MMRAcquisitionAuctionCleanPrice          28\n",
       "MMRAcquisitionRetailAveragePrice         28\n",
       "MMRAcquisitonRetailCleanPrice            28\n",
       "MMRCurrentAuctionAveragePrice           458\n",
       "MMRCurrentAuctionCleanPrice             458\n",
       "MMRCurrentRetailAveragePrice            458\n",
       "MMRCurrentRetailCleanPrice              458\n",
       "PRIMEUNIT                            115755\n",
       "AUCGUART                             115755\n",
       "BYRNO                                     0\n",
       "VNZIP1                                    0\n",
       "VNST                                      0\n",
       "VehBCost                                  0\n",
       "IsOnlineSale                              0\n",
       "WarrantyCost                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e14a4a6c7dc7d5fbd21e58e43d71256357733f4"
   },
   "source": [
    "###  Drop the unimportant stuff\n",
    "\n",
    "There are too many missing data in \"PRIMEUNIT\" and \"AUCGUART\", so we can just remove these features.\n",
    "Also \"RefId\" has nothing to do but only the ID number that can be removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b37b06030bc2581722bd2d98752e498fc5b6627f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_data.drop(labels='AUCGUART', axis='columns', inplace=True)\n",
    "all_data.drop(labels='PRIMEUNIT', axis='columns', inplace=True)\n",
    "all_data.drop(labels='RefId', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e55734ef4c3fa2460d01deaa351113e66c5ee74"
   },
   "source": [
    "### Deal with missing values\n",
    "\n",
    "<p>Let's take a look at what missing values we'll have to handle.</p> For simplicity, just remove all columns that has null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PurchDate       0\n",
       "Auction         0\n",
       "VehYear         0\n",
       "VehicleAge      0\n",
       "Make            0\n",
       "Model           0\n",
       "VehOdo          0\n",
       "BYRNO           0\n",
       "VNZIP1          0\n",
       "VNST            0\n",
       "VehBCost        0\n",
       "IsOnlineSale    0\n",
       "WarrantyCost    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_isnull = all_data.columns[all_data.isnull().any()]\n",
    "for col in all_isnull:\n",
    "    all_data.drop(labels=col, axis='columns', inplace=True)\n",
    "all_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with category variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PurchDate', 'Auction', 'Make', 'Model', 'VNST']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['MMRCurrentRetailCleanPrice'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-204483936a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnonList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mall_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3697\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4404\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['MMRCurrentRetailCleanPrice'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nonList = list(all_data.select_dtypes(include=[np.object]))\n",
    "print (nonList)\n",
    "\n",
    "for this_label in nonList:\n",
    "    all_data.drop(labels=this_label, axis='columns', inplace=True)\n",
    "all_data2 = all_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da3da259df43ee819735f558d677348c33641d49"
   },
   "source": [
    "###  Diving in (machine learning)\n",
    "\n",
    "<p>Now that the data has been cleaned, we can try to find a model that works well for making our predictions. We'll also load in some classifiers which we will compare.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45e81048fd2b50e44ca687582f9900520d1e6392",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "Xtrain = all_data2.head(len(Ytrain))\n",
    "\n",
    "RFC = RandomForestClassifier()\n",
    "Ada = AdaBoostClassifier()\n",
    "KNN = KNeighborsClassifier()\n",
    "classifiers = [RFC, Ada, KNN]\n",
    "clf_names = ['Random Forest', 'AdaBoost', 'K Nearest Neighbors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2573510f0de967cf3741b0963345784f32caaa30"
   },
   "source": [
    "<p>For this analysis, we'll only be comparing across three classifiers: Random Forest, AdaBoost, and K Nearest Neighbors. For more information on other potential (or more complicated) classifiers I invite you to check out the other kernels posted by those who top the leaderboards for this competition.</p>\n",
    "\n",
    "<p>For each of these classifiers, we'll want to make sure we create the models with the optimal parameters. We can do this with a Grid Search. We define the set of parameters we want to scan for each type of classifier, and then run our grid searches.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a322b47a7dc1d0147ffbd3fcd3241366fe438af6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "\n",
    "# Use kfold as our cross validation\n",
    "kfold = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# Set grid search parameter settings\n",
    "rfc_param_grid = {'max_depth': [None],\n",
    "                 'max_features': [1],\n",
    "                 'min_samples_split': [2],\n",
    "                 'min_samples_leaf': [1],\n",
    "                 'bootstrap': [False],\n",
    "                 'n_estimators': [100],\n",
    "                 'criterion': ['gini']}\n",
    "ada_param_grid = {'n_estimators': [20],\n",
    "                 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "knn_param_grid = {'n_neighbors': [5],\n",
    "                  'weights': ['uniform', 'distance'],\n",
    "                 'leaf_size': [5]}\n",
    "param_grids = [rfc_param_grid, ada_param_grid, knn_param_grid]\n",
    "\n",
    "# Perform grid searches to get estimators with the optimal settings\n",
    "grid_searches = []\n",
    "for i in range(len(classifiers)):\n",
    "    grid_searches.append(GridSearchCV(estimator=classifiers[i], param_grid=param_grids[i], \n",
    "                                      n_jobs=4, cv=kfold, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d632d4a7bf7c566b139aa09ac56d12cbfdf10116"
   },
   "source": [
    "<p>We'll now want to see the training scores for each of our models and determine which one works the best. We'll fit each model to our training set and add the best scores from each to a list.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f8452618656ef1a841d1a7ce1f69478f0d963ef"
   },
   "outputs": [],
   "source": [
    "# Train the models\n",
    "best_scores = []\n",
    "for i in range(len(grid_searches)):\n",
    "    grid_searches[i].fit(Xtrain, Ytrain)\n",
    "    best_scores.append(grid_searches[i].best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f499d641ff0414737da2c8e64339934c034dd968"
   },
   "source": [
    "<p>Let's see the best scores for each classifier.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "242a20ab1d09bd77b8082086be53cff9c12ce541"
   },
   "outputs": [],
   "source": [
    "# Best scores\n",
    "for i in range(len(best_scores)):\n",
    "    print(clf_names[i] + \": \" + str(best_scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "763ba583c7e203ca3d44f39ce89cfd6d7c0c0e5a"
   },
   "source": [
    "<p>Based on these training scores, it makes the most sense to use the Random Forest Classifier to make the predictions. We'll predict on the test set, and then write the predictions to a csv file for submission.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3ca1c1802687ab5e165f0dfcbcc4e06a574604f"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Xtest = all_data2.tail(ntest)\n",
    "#Xtest = test_df.drop(columns='RefId', axis='columns')\n",
    "predictions = grid_searches[0].predict(Xtest)\n",
    "\n",
    "# Write predictions to output csv\n",
    "pred_df = pd.DataFrame({'RefId': test_df['RefId'],\n",
    "                        'IsBadBuy': predictions})\n",
    "pred_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Done writing to csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
